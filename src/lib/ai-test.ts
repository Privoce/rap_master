import { requestAILocal, getLocalModels } from '@/lib/ai';

/**
 * æµ‹è¯• AI API å®¢æˆ·ç«¯
 * è¿è¡Œæ­¤æ–‡ä»¶æ¥éªŒè¯æœ¬åœ° Ollama è¿æ¥æ˜¯å¦æ­£å¸¸
 */

export async function testAIConnection() {
  console.log('ğŸ” å¼€å§‹æµ‹è¯• AI API è¿æ¥...\n');

  try {
    // æµ‹è¯• 1: è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨
    console.log('ğŸ“‹ æµ‹è¯• 1: è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨');
    const modelsResponse = await getLocalModels();

    if (modelsResponse.success) {
      console.log('âœ… æœ¬åœ°æ¨¡å‹åˆ—è¡¨è·å–æˆåŠŸ:');
      console.log(JSON.stringify(modelsResponse.data, null, 2));
    } else {
      console.log('âŒ æœ¬åœ°æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥:', modelsResponse.error);
    }

    console.log('\n' + '='.repeat(50) + '\n');

    // æµ‹è¯• 2: ç®€å•æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚æœæœ‰å¯ç”¨æ¨¡å‹ï¼‰
    if (modelsResponse.success && modelsResponse.data?.models?.length > 0) {
      const availableModel = modelsResponse.data.models[0].name;
      console.log(`ğŸ“ æµ‹è¯• 2: ä½¿ç”¨æ¨¡å‹ "${availableModel}" ç”Ÿæˆæ–‡æœ¬`);

      const generateResponse = await requestAILocal({
        model: availableModel,
        prompt: 'è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±',
        options: {
          temperature: 0.7,
          num_predict: 50, // é™åˆ¶è¾“å‡ºé•¿åº¦
        },
      });

      if (generateResponse.success) {
        console.log('âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸ:');
        console.log(generateResponse.data.response || generateResponse.data);
      } else {
        console.log('âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥:', generateResponse.error);
      }
    } else {
      console.log('âš ï¸  è·³è¿‡æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼ˆæ— å¯ç”¨æœ¬åœ°æ¨¡å‹ï¼‰');
    }

    console.log('\n' + '='.repeat(50) + '\n');

    // æµ‹è¯• 3: å¯¹è¯æ¨¡å¼ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if (modelsResponse.success && modelsResponse.data?.models?.length > 0) {
      const availableModel = modelsResponse.data.models[0].name;
      console.log(`ğŸ’¬ æµ‹è¯• 3: ä½¿ç”¨æ¨¡å‹ "${availableModel}" è¿›è¡Œå¯¹è¯`);

      const chatResponse = await requestAILocal({
        model: availableModel,
        messages: [
          { role: 'user', content: 'ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±' },
        ],
        options: {
          temperature: 0.7,
          num_predict: 100,
        },
      });

      if (chatResponse.success) {
        console.log('âœ… å¯¹è¯æµ‹è¯•æˆåŠŸ:');
        console.log(chatResponse.data.response || chatResponse.data.message?.content || chatResponse.data);
      } else {
        console.log('âŒ å¯¹è¯æµ‹è¯•å¤±è´¥:', chatResponse.error);
      }
    }

  } catch (error) {
    console.error('ğŸ”¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:', error);
  }

  console.log('\nğŸ AI API è¿æ¥æµ‹è¯•å®Œæˆ');
}

/**
 * å¿«é€Ÿæµ‹è¯•å‡½æ•°ï¼ˆä»…æ£€æŸ¥è¿æ¥ï¼‰
 */
export async function quickTest() {
  try {
    const response = await getLocalModels();
    if (response.success) {
      console.log('âœ… Ollama è¿æ¥æ­£å¸¸');
      return true;
    } else {
      console.log('âŒ Ollama è¿æ¥å¤±è´¥:', response.error);
      return false;
    }
  } catch (error) {
    console.log('âŒ è¿æ¥æµ‹è¯•å‡ºé”™:', error);
    return false;
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œæµ‹è¯•
if (require.main === module) {
  testAIConnection().catch(console.error);
}

export default {
  testAIConnection,
  quickTest,
};
